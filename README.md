# awesome-chatgpt-dataset
![Alt Text](https://github.com/voidful/awesome-chatgpt-dataset/raw/main/A%20cat%20%20to%20Unlock%20the%20Power%20of%20LLM%20Explore%20These%20Datasets%20to%20Train%20Your%20Own%20ChatGPT!.gif)    

## Unlock the Power of LLM: Explore These Datasets to Train Your Own ChatGPT!

| Dataset Name                                                                                                      | Size | Languages                                                          | Source                                                                                                                                                                                                                                | License                                                                          |
|-------------------------------------------------------------------------------------------------------------------|------|--------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| [cc_sbu_align](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align)                                          | 4K   | English                                                            | MiniGPT-4  datadset                                                                                                                                                                                                                   | BSD 3-Clause License                                                             |
| [blended_skill_talk](https://huggingface.co/datasets/blended_skill_talk)                                          | 7K   | English                                                            | A dataset of 7k conversations explicitly designed to exhibit multiple conversation modes: displaying personality, having empathy, and demonstrating knowledge.                                                                        | -                                                                                |
| [GSM-IC](https://github.com/google-research-datasets/GSM-IC)                                                      | 8K   | English                                                            | Grade-School Math with Irrelevant Context (GSM-IC)                                                                                                                                                                                    | -                                                                                |
| [ChatAlpaca](https://github.com/cascip/ChatAlpaca)                                                                | 10K  | English                                                            | The data currently contain a total of 10,000 conversations with 95,558 utterances.                                                                                                                                                    |  Apache-2.0 license                                                              |
| [Dolly](https://github.com/databrickslabs/dolly/tree/master/data)                                                 | 15K  | English                                                            | databricks-dolly-15k is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language models to exhibit the magical interactivity of ChatGPT.                                          | CC 3.0                                                                           |
| [WebGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)                                               | 20K  | English                                                            | This is the dataset of all comparisons that were marked as suitable for reward modeling by the end of the WebGPT project.                                                                                                             | -                                                                                |
| [Code Alpaca](https://github.com/sahil280114/codealpaca)                                                          | 20K  | English                                                            | Code generation task involving 20,022 samples                                                                                                                                                                                         | -                                                                                |
| [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)                                                         | 37K  | English, Chinese                                                   | 37,175 instructions generated by ChatGPT and human                                                                                                                                                                                    | -                                                                                |
| [Alpaca Dataset](https://github.com/tatsu-lab/stanford_alpaca)                                                    | 52K  | English                                                            | 175 seed instructions by OpenAI API                                                                                                                                                                                                   | CC By NC 4.0; OpenAI terms of use                                                |
| [Alpaca Data Cleaned](https://github.com/gururise/AlpacaDataCleaned)                                              | 52K  | English                                                            | Revised version of Alpaca Dataset                                                                                                                                                                                                     | -                                                                                |
| [Alpaca GPT-4 Data](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                                   | 52K  | English                                                            | Generated by GPT-4 using Alpaca prompts                                                                                                                                                                                               | -                                                                                |
| [Alpaca GPT-4 Data (Chinese)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                         | 52K  | Chinese                                                            | Generated by GPT-4 using Chinese prompts translated from Alpaca by ChatGPT                                                                                                                                                            | -                                                                                |
| [Cabrita Dataset](https://github.com/22-hours/cabrita)                                                            | 52K  | Portuguese                                                         | Translated from Alpaca Data                                                                                                                                                                                                           |                                                                                  |
| [Japanese Alpaca Dataset](https://github.com/shi3z/alpaca_ja)                                                     | 52K  | Japanese                                                           | Translated from Alpaca Data by ChatGPT API                                                                                                                                                                                            | CC By NC 4.0; OpenAI terms of use                                                |
| [Traditional Chinese Alpaca Dataset](https://github.com/ntunlplab/traditional-chinese-alpaca)                     | 52K  | Traditional Chinese                                                | Translated from Alpaca Data by ChatGPT API                                                                                                                                                                                            | Apache-2.0 license                                                               |
| [Finance](https://huggingface.co/datasets/gbharti/finance-alpaca)                                                 | 69K  | English                                                            | 68,912 financial related instructions                                                                                                                                                                                                 | -                                                                                |
| [evol](https://huggingface.co/datasets/victor123/evol_instruct_70k)                                               | 70K  | English                                                            | This is the training data of WizardLM.                                                                                                                                                                                                | -                                                                                |
| [Vicuna Dataset](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)                       | 75K  | English                                                            | ~100k ShareGPT conversations                                                                                                                                                                                                          | -                                                                                |
| [InstructionTranslation](https://huggingface.co/datasets/theblackcat102/instruction_translations)                 | 80K  | Multi-lingual                                                      | Translations were generated by M2M 12B and the output generations were limited at 512 tokens due to VRAM limit (40G).                                                                                                                 | MIT                                                                              |
| [Self-Instruct](https://github.com/yizhongw/self-instruct/tree/main)                                              | 82K  | English                                                            | We release a dataset that contains 52k instructions, paired with 82K instance inputs and outputs.                                                                                                                                     | -                                                                                |
| [OASST1](https://huggingface.co/datasets/OpenAssistant/oasst1)                                                    | 89K  | Multi-lingual                                                      | a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. | apache-2.0                                                                       |
| [HH-RLHF](https://github.com/anthropics/hh-rlhf/tree/master)                                                      | 91K  | English                                                            | The data are described in the paper: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.                                                                                                       | MIT                                                                              |
| [Guanaco Dataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)                                  | 98K  | English, Simplified Chinese, Traditional Chinese HK & TW, Japanese | 175 tasks from the Alpaca model                                                                                                                                                                                                       | GPLv3                                                                            |
| [InstructionWild](https://github.com/XueFuzhao/InstructionWild)                                                   | 104K | English, Chinese                                                   | 429 seed instructions and follow Alpaca to generate 52K                                                                                                                                                                               | Research only; OpenAI terms of use                                               |
| [Camel Dataset](https://github.com/lightaime/camel)                                                               | 107K | Multi-lingual                                                      | Role-playing between AIs (Open AI API)                                                                                                                                                                                                | -                                                                                |
| [LLaVA Visual Instruct](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)                           | 150K | English                                                            | LLaVA Visual Instruct 150K is a set of GPT-generated multimodal instruction-following data. It is constructed for visual instruction tuning and for building large multimodal towards GPT-4 vision/language capability.               | cc-by-nc-4.0                                                                     |
| [Prosocial Dialog](https://huggingface.co/datasets/allenai/prosocial-dialog)                                      | 166K | English                                                            | 165,681 instructions produced by GPT-3 rewrites questions and human feedback                                                                                                                                                          | -                                                                                |
| [COIG](https://huggingface.co/datasets/BAAI/COIG)                                                                 | 191K | Chinese                                                            | Chinese Open Instruction Generalist (COIG) project to maintain a harmless, helpful, and diverse set of Chinese instruction corpora.                                                                                                   | apache-2.0                                                                       |
| [Unnatural Instructions](https://github.com/orhonovich/unnatural-instructions)                                    | 241K | English                                                            | a large dataset of cre- ative and diverse instructions, collected with virtually no human labor.                                                                                                                                      | MIT                                                                              |
| [SHP](https://huggingface.co/datasets/stanfordnlp/SHP)                                                            | 358K | English                                                            | SHP is a dataset of 385K collective human preferences over responses to questions/instructions in 18 different subject areas, from cooking to legal advice.                                                                           | Reddit non-exclusive, non-transferable, non-sublicensable, and revocable license |
| [ultrachat](https://huggingface.co/datasets/stingning/ultrachat)                                                  | 404K | English                                                            | To ensure generation quality, two separate ChatGPT Turbo APIs are adopted in generation, where one plays the role of the user to generate queries and the other generates the response.                                               | cc-by-nc-4.0                                                                     |
| [ELI5](https://huggingface.co/datasets/eli5)                                                                      | 559K | English                                                            | The ELI5 dataset is an English-language dataset of questions and answers gathered from three subreddits where users ask factual questions requiring paragraph-length or longer answers.                                               | -                                                                                |
| [GPT4All Dataset](https://github.com/nomic-ai/gpt4all)                                                            | 806K | Multi-lingual                                                      | Subset of LAION OIG, StackOverflow Question, BigSciense/p3 dataset. Answered by OpenAI API.                                                                                                                                           | -                                                                                |
| [Instruct](https://huggingface.co/datasets/swype/instruct)                                                        | 889K | English                                                            | 888,969 English instructions, augmentation using AllenAI NLP tools                                                                                                                                                                    | MIT                                                                              |
| [MOSS](https://github.com/OpenLMLab/MOSS#数据)                                                                      | 1M   | Chinese                                                            | Generated by gpt-3.5-turbo                                                                                                                                                                                                            | Apache-2.0, AGPL-3.0 licenses                                                    |
| [LaMini-Instruction](https://huggingface.co/datasets/MBZUAI/LaMini-instruction)                                   | 3M   | English                                                            | a total of 2.58M pairs of instructions and responses using gpt-3.5-turbo based on several existing resources of prompts                                                                                                               | cc-by-nc-4.0                                                                     |
| [Natural Instructions](https://github.com/allenai/natural-instructions)                                           | 5M   | Multi-lingual                                                      | 5,040,134 instructions collected from diverse NLP tasks                                                                                                                                                                               | -                                                                                |
| [BELLE](https://github.com/LianjiaTech/BELLE/tree/main/data)                                                      | 10M  | Chinese                                                            | The 10M Chinese dataset is composed of subsets spanning multiple (instruction) types and multiple fields.                                                                                                                             | Research only; OpenAI terms of use                                               |
| [Firefly](https://github.com/yangjianxin1/Firefly)                                                                | 16M  | Chinese                                                            | 1,649,398 Chinese instructions in 23 NLP tasks                                                                                                                                                                                        | -                                                                                |
| [OIG-43M Dataset](https://laion.ai/blog/oig-dataset/)                                                             | 43M  | Multi-lingual                                                      | Together, LAION, and Ontocord.ai.                                                                                                                                                                                                     | -                                                                                |
| [xP3](https://huggingface.co/datasets/bigscience/xP3)                                                             | 79M  | Multi-lingual                                                      | 78,883,588 instructions collected by prompts & datasets across 46 languages & 16 NLP tasks                                                                                                                                            | -                                                                                |
| [Alpaca-CoT Dataset](https://github.com/PhoebusSi/Alpaca-CoT/tree/main/data)                                      | -    | Multi-lingual                                                      | Instruction Data Collection                                                                                                                                                                                                           | ODC-By                                                                           |
| [stack-exchange-paired](https://huggingface.co/datasets/lvwerra/stack-exchange-paired)                            | -    | English                                                            | This dataset contains questions and answers from the Stack Overflow Data Dump for the purpose of preference model training.                                                                                                           | cc-by-sa-4.0                                                                     |
| [CodeParrot](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot#dataset) | -    | python                                                             | The database was queried for all Python files with less than 1MB in size resulting in a 180GB dataset with over 20M files.                                                                                                            | -                                                                                |
| [LangChainDatasets](https://huggingface.co/LangChainDatasets)                                                     | -    | English                                                            | This is a community-drive dataset repository for datasets that can be used to evaluate LangChain chains and agents.                                                                                                                   | -                                                                                |
| [ParlAI](https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks)                                       | -    | English                                                            | 100+ popular datasets available all in one place, dialogue models, from open-domain chitchat, to task-oriented dialogue, to visual question answering.                                                                                | -                                                                                |
| [GPTeacher](https://github.com/teknium1/GPTeacher)                                                                | -    | English                                                            | A collection of modular datasets generated by GPT-4, General-Instruct - Roleplay-Instruct - Code-Instruct - and Toolformer                                                                                                            | -                                                                                |
